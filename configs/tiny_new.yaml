general:
    wandb:
        project: pixel_titok_video_9
        run_name: GAN-BS8-DFF-transAndNAF-8F-larpPixelLoss-600kMaxSteps-1FusionLayer-1SepLayer-tiny_thin_512_halfMLP
        log_step_interval: 50

    checkpoints:
        save_path: out_tiny_w512_exp_3
        save_interval: 1000
        keep_prior: 2 # -1 to keep all

        resume_from_checkpoint:
        init_from_checkpoint:

model:
    resnaf:
        temporal_patch_size: 4
        spatial_patch_size: 8

        sep_layers: 1 # 2 in leanVAE, use here too for final?
        fusion_layers: 1 # 4 in leanVAE, transformer will make up? Use 2 for final?

        low_dims: 128
        high_dims: 384

    titok:
        encoder_size: tiny_thin # tiny
        decoder_size: tiny_thin

        num_latent_tokens: 256 # 512
        fsq_levels: [7, 5, 5, 5, 5]

    disc:
        temporal_patch_size: 4
        spatial_patch_size: 8

        model_size: tiny_thin # tiny

losses:
    recon:
        dwt_weight: 0.1
        perceptual_weight: 1.0
        perceptual_subsample: 3 # calculate lpips score from lpips_subsample frames in each clip. Use -1 to sample all.

    disc:
        use_disc: False
        freeze_encoder: False

        disc_start: 294000
        disc_weight: 0.4
        disc_weight_warmup_steps: 1

        every_n: 7 # num encoder steps between discriminator steps
        base_gamma: 500.0
        final_gamma: 50.0

dataset:
    train_dataset: "hf://datasets/NilanE/Vchitect_T2V_DataVerse_256p_8fps_wds/shards/*.tar" # "hf://datasets/facebook/PE-Video/train/*.tar"
    eval_dataset: "eval_dataset/03.tar" # "hf://datasets/facebook/PE-Video/test/*.tar"
    resolution: 128
    num_frames: 8 # 16
    frames_per_second: 8
    workers: 2 # 4
    pin_memory: False # uses extra VRAM when enabled.

    chunk_type: random_multi # random_multi, random_single
    max_chunks: 20 # avoid OOM

optimizer:
    titok:
        learning_rate: 5e-5 # 1e-4
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-4 # 1e-4 -> 0.0001
        warmup_steps: 2000
        end_lr: 1e-5  # 1e-5

    disc:
        learning_rate: 1e-6 # 5e-6 | 1e-5 | See https://arxiv.org/pdf/2501.09755
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-4 # lower?
        warmup_steps: 0 # 10000 # 20000
        end_lr: 2e-7 # 5e-6 | 1e-6 | 5x less than LR

training:
    main:
        max_steps: 600000
        batch_size: 8
        precision: bf16-mixed
        accelerator: 'gpu'
        train_devices: 1
        enable_tf32: True
        torch_compile: False

        seed: 42
        max_grad_norm: 1.0

    eval:
        eval_step_interval: 500 # 1000
        num_eval: 128
        batch_size: 4

        log_codebook: True
        log_jedi: True
        log_fvd: True
        log_recon_num: 4

        random_recon: False # whether to sample recon videos randomly from the eval set, or take the first log_recon_num
        jedi_jepa_model: vit_large # vit_huge

        clear_cache: True
        offload_metrics: True