general:
    wandb:
        project: pixel_titok_video_11
        run_name: LPIPS144x224-SL172k-BaseLarge-1024Eval-smallGAN0.2-sameLR-N1-NWW5k-DWW60k-DOW60k-EMA-freezeEnc-bigLR
        log_step_interval: 50

    checkpoints:
        save_path: out_ckpt
        save_interval: 2000
        keep_prior: 2 # -1 to keep all

        resume_from_checkpoint: out_ckpt/epoch=0-step=198000.ckpt # 118k-multires-large.ckpt
        init_from_checkpoint:

model:
    titok:
        patch_size: [4, 8, 8]

        encoder_size: base_thin
        decoder_size: large_thin

        fsq_levels: [7, 7, 7, 5, 5] # CB size: 8575

    disc:
        patch_size: [4, 8, 8]

        model_size: small_thin

losses:
    recon:
        perceptual_weight: 1.0 # 1.0
        perceptual_samples_per_step: 224 # Use -1 to sample all.
        perceptual_sampling_size: 144 # fixed-size input to LPIPS. Always square.

    disc:
        use_disc: True
        freeze_encoder: True

        disc_start: 118000
        disc_warmup_steps: 5000 # no GAN loss applied to the tokenizer during this period, but disc is still trained.
        disc_weight: 0.2
        disc_weight_warmup_steps: 60000

        every_n: 1 # num encoder steps between discriminator steps
        lecam_weight: 0.0 # 0.001
        adaptive_weight: False

dataset:
    # train_dataset: "hf://datasets/NilanE/Vchitect_T2V_DataVerse_256p_8fps_wds/shards/*.tar" # "hf://datasets/facebook/PE-Video/train/*.tar"
    # eval_dataset: "hf://datasets/facebook/PE-Video/test/*.tar"

    train_dataset: "ds/shards/{00000..00079}.tar"
    eval_dataset: "ds/test/{000000..000029}.tar" # "hf://datasets/facebook/PE-Video/test/{000000..000029}.tar" # 

    fps_range: [3, 4]
    max_aspect_ratio: 2

    workers: 8
    pin_memory: True # uses extra VRAM when enabled.

optimizer:
    titok:
        learning_rate: 5e-5
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-4
        warmup_steps: 10000
        end_lr: 1e-5

    disc:
        learning_rate: 5e-5 # 5e-6 # See https://arxiv.org/pdf/2501.09755
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-4
        warmup_steps: 60000
        end_lr: 1e-5 # 1e-6

training:
    sampling:
        num_token_range: [128, 1024]
        min_grid: [8, 128, 128]
        max_grid: [20, 192, 192]

        # seq_len is similar to batch size, but raw token count.
        # Set as much as the GPU can handle. The dataloader will try to target this value, but never exceed.
        train_seq_len: 172032 # 147456 # per-gpu or all?
        eval_seq_len: 32768

    main:
        max_steps: 600000
        precision: bf16-mixed
        accelerator: 'gpu'
        train_devices: 1
        enable_tf32: True
        torch_compile: False # True

        seed: 42
        max_grad_norm: 1.0

        use_ema: True # enable alongside disc model
        ema_decay: 0.999 # vitok uses 0.9999

    eval:
        eval_step_interval: 2000
        num_eval: 1024

        log_codebook: True
        log_jedi: True
        log_fvd: True
        log_recon_num: 16

        random_recon: True # whether to sample recon videos randomly from the eval set, or take the first log_recon_num
        jedi_jepa_model: vit_huge # vit_large, vit_huge

        clear_cache: True
