logging:
    project: titok_video_speedy_RGB_image_4
    run_name: TL32-BS64-baseline-smallDec-dreamsimopenClip320.8-WD1e-2

    logging_interval: 50

    save_path: out_tiny_dreamsim
    save_step_interval: 1000
    keep_prior_checkpoints: 2 # -1 to keep all, anything else only keeps the first N and doesn't save any past that...

    resume_from_checkpoint:
    init_from_checkpoint: baseline-smallDec-40k.ckpt

model:
    titok:
        spatial_patch_size: 16 # 64 internal tokens @ 128p

        fsq_levels: [3, 2, 2, 2, 2]
        num_latent_tokens: 32
        # also compare to width increase that keeps the same data rate, but allocated to width rather than extra tokens?

        encoder_size: tiny
        decoder_size: small
        exp_residual: False

        lpips_weight: 0.8 # 1.0 for dreamsim? # 0.5 | 0.0 = disabled | use lower for dists? Or warmup to keep grad norm low?

        vmaf_weight: 0.0 # 0.05 | 0.1 | lower to avoid artifacts? - start delayed
        vmaf_neg: False

        dwt_weight: 0.0 # 3.0 | increase to 2 or 3x since base range ~0.2 max, is small.

    disc: # experimental
        use_disc: False

        disc_layers: 1 # 1
        disc_filters: 64 # 32

        disc_start: 500 # use 5000 for stability?
        disc_factor: 1.0
        disc_weight: 0.3 # 0.01
        lecam_weight: 0.0 # 0.0 = disabled, 0.001 normal

        base_gamma: 1.0  # higher gamma smooths more earlier in training.
        final_gamma: 0.1

        every_n: 1 # train disc every n steps of generator
        adapt_disc_weight: False

dataset:
    train_dataset: "in_1k/train/imagenet1k-train-{0000..0239}.tar"
    eval_dataset: "in_1k/eval/imagenet1k-validation-00.tar"
    resolution: 128
    workers: 4
    pin_memory: False

optimizer:
    titok:
        learning_rate: 1e-4
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-2 # 1e-4 -> 0.0001
        warmup_steps: 2000 # 2000
        end_lr: 1e-5

    disc:
        learning_rate: 1e-4
        beta1: 0.9
        beta2: 0.99
        weight_decay: 1e-4
        warmup_steps: 2000
        end_lr: 1e-5

training:
    torch_compile: False
    seed: 42
    max_grad_norm: 1.0
    
    batch_size: 64
    enable_tf32: True
    precision: bf16-mixed
    train_devices: 1
    accelerator: 'gpu'

    max_steps: 150000
    val_step_interval: 250

    eval_recon_log_num: 4 # should be less than eval_sample_size
    eval_sample_size: 256
    eval_batch_size: 16
    eval_clear_cache: True

    log_codebook: True