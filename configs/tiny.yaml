general:
    wandb:
        project: pixel_titok_video_12
        run_name: LPIPS128x24-SL6k-tiny-NoEval-r3GAN0.4-NoWarm-15thLR-noMomentum-noEMA-GP0.3-FF
        log_step_interval: 50

    checkpoints:
        save_path: out_tiny_w512_exp_3
        save_interval: 1000
        keep_prior: 2 # -1 to keep all

        resume_from_checkpoint:
        init_from_checkpoint:

tokenizer:
    model:
        patch_size: [4, 8, 8]
        fsq_levels: [7, 5, 5, 5, 5]
        encoder_size: tiny_thin
        decoder_size: tiny_thin

    losses:
        perceptual_weight: 1.0 # 1.0
        perceptual_samples_per_step: 24 # Use -1 to sample all.
        perceptual_sampling_size: 128 # fixed-size input to LPIPS. Always square.

    optimizer:
        lr_schedule: cosine # cosine, constant
        learning_rate: 1e-4
        end_lr: 1e-5
        warmup_steps: 1000

        beta1: 0.0 # disable momentum for GAN stability
        beta2: 0.99
        weight_decay: 1e-4

discriminator:
    use_disc: True # whether to init disc weights and optimizer. Can be enabled on resume from pretrained non-disc ckpt.

    model:
        patch_size: [4, 8, 8]
        model_size: tiny_thin

    losses:
        disc_start: 0 # 10000
        disc_warmup_steps: 1 # no GAN loss applied to the tokenizer during this period, but disc is still trained.
        disc_weight: 0.4 # lower to 0.2 - and lower warmup accordingly to ~100k?
        disc_weight_warmup_steps: 1 # 10000 # 105000 # 160000

        lecam_weight: 0.0 # 0.001
        gradient_penalty_weight: 0.3
        gradient_penalty_noise: 0.01 # 0.1?

    optimizer:
        lr_divisor: 15 # how many times lower the disc LR is than the tokenizer at a given step
        warmup_steps: 1 # 5000 # 85000 # disc_weight_warmup_steps + disc_warmup_steps

        beta1: 0.0
        beta2: 0.99
        weight_decay: 1e-4

dataset:
    train_dataset: "hf://datasets/NilanE/Vchitect_T2V_DataVerse_256p_8fps_wds/shards/{00000..00079}.tar"
    eval_dataset: "hf://datasets/facebook/PE-Video/test/{000000..000029}.tar"

    fps_range: [3, 4]
    max_aspect_ratio: 2

    workers: 3
    pin_memory: False # uses extra VRAM when enabled.


training:
    sampling:
        num_token_range: [64, 384]
        min_grid: [4, 96, 96]
        max_grid: [12, 160, 160]

        # seq_len is similar to batch size, but raw token count.
        # Set as much as the GPU can handle. The dataloader will try to target this value, but never exceed.
        train_seq_len: 6144
        eval_seq_len: 4096

    main:
        max_steps: 600000
        precision: bf16-mixed
        accelerator: 'gpu'
        train_devices: 1
        enable_tf32: True
        torch_compile: False

        seed: 42
        max_grad_norm: 1.0

        use_ema: False # enable alongside disc model
        ema_decay: 0.999 # vitok uses 0.9999

    eval:
        eval_step_interval: 600000 # 1000
        num_eval: 1024

        log_codebook: True
        log_jedi: False
        log_fvd: True
        log_recon_num: 16

        random_recon: True # whether to sample recon videos randomly from the eval set, or take the first log_recon_num
        jedi_jepa_model: vit_huge # vit_large, vit_huge

        clear_cache: True